# -*- coding: utf-8 -*-
"""Pytorch Prototyping ~ MNIST - Workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ww0g8U7dFGo9FY1N548219r5kRv0dj1q
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader , TensorDataset
from torchvision import datasets , transforms

class neural_network(nn.Module):

  def __init__(self, input_dim , hidden_dim , output_dim):

    super().__init__()

    self.net = nn.Sequential(
        nn.Linear(input_dim , hidden_dim),
        nn.ReLU(),
        nn.Linear(hidden_dim, output_dim)
    )

  def forward(self , X):
    return self.net(X)

transform = transforms.ToTensor()

train_data = datasets.MNIST(root='./data' , train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='./data' , train=False, download=True, transform=transform)

train_loader = DataLoader(train_data , batch_size=64 , shuffle=True)
test_loader = DataLoader(test_data, batch_size=64)

class digital_classifier(nn.Module):
  def __init__(self):
    super().__init__()
    self.net = nn.Sequential(
        nn.Flatten(),
        nn.Linear(784 , 128), # input --> hidden
        nn.ReLU(),
        nn.Linear(128 , 10) # hidden --> output (10 Classes)
    )

  def forward(self , X):
    return self.net(X)

model = digital_classifier()

loss_fn = nn.CrossEntropyLoss() # used for multi-class classification

optimizer = optim.Adam(model.parameters() , lr=0.001)

for epoch in range(40):

  model.train()

  total_loss= 0

  for batch_X , batch_y in train_loader:
    optimizer.zero_grad()
    output = model(batch_X)
    loss = loss_fn(output , batch_y)
    loss.backward()
    optimizer.step()
    total_loss += loss.item()


  print(f'Epoch: {epoch+1}, Loss: {total_loss:.4f}')


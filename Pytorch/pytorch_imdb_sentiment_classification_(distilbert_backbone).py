# -*- coding: utf-8 -*-
"""PyTorch IMDb Sentiment Classification (DistilBERT backbone).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zDEc5sYNpIfV-rRHcc4zl6hqNJTPlDfc
"""

!pip install pip install transformers datasets

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

dataset = load_dataset('imdb')

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize(batch): return tokenizer(batch['text'] , padding='max_length' , truncation=True , max_length=256)

tokenized = dataset.map(
    tokenize,
    batched=True
)

import torch
from torch.utils.data import Dataset , DataLoader

class text_data(Dataset):
  def __init__(self , encodings , labels):
    self.encodings = encodings
    self.labels = labels

  def __getitem__(self, idx):
    # Access individual encoding components by key for the given index
    item = {
        'input_ids': torch.tensor(self.encodings['input_ids'][idx]),
        'attention_mask': torch.tensor(self.encodings['attention_mask'][idx])
        # Add other encoding components if necessary
    }
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self): return len(self.labels)


train_dataset = text_data(tokenized['train'] , tokenized['train']['label'])
test_dataset  = text_data(tokenized['test'] , tokenized['test']['label'])


train_loader = DataLoader(train_dataset , batch_size=8 , shuffle=True)
test_loader = DataLoader(test_dataset , batch_size=8)

train_dataset = text_data(tokenized['train'] , tokenized['train']['label'])
test_dataset  = text_data(tokenized['test'] , tokenized['test']['label'])

train_loader = DataLoader(train_dataset , batch_size=8 , shuffle=True)
test_loader = DataLoader(test_dataset , batch_size=8)

from transformers import AutoModelForSequenceClassification

model_1 = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased' , num_labels=2)
model_1 = model_1.to('cuda' if torch.cuda.is_available() else 'cpu')

import torch
from torch import nn, optim

optimizer = optim.AdamW(model_1.parameters(), lr=5e-5)
loss_fn = nn.CrossEntropyLoss()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for epoch in range(2):  # 2 epochs for demo
    model_1.train()
    total_loss = 0

    for batch in train_loader:
        # Move to device
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        # Forward
        outputs = model_1(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        loss = loss_fn(logits, labels)

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")

from sklearn.metrics import accuracy_score

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

acc = accuracy_score(all_labels, all_preds)
print(f"Test Accuracy: {acc:.4f}")
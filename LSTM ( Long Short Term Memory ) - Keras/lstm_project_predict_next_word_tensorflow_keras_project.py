# -*- coding: utf-8 -*-
"""Lstm_project_predict_next_word_tensorflow_keras_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msbbpnIGTTI9mpNPtF0CUQ-wZQeUi_IT
"""

import pandas as pd,  numpy as np

doc = """
Machine learning is a subset of artificial intelligence that enables systems to learn from data and improve performance over time without being explicitly programmed. It is widely used in various industries, including finance, healthcare, e-commerce, and transportation. Machine learning algorithms can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.

Supervised learning involves training a model on labeled data. In this approach, the algorithm learns to map inputs to outputs based on example input-output pairs. Common applications of supervised learning include regression and classification tasks, such as predicting stock prices or diagnosing diseases based on medical records.

Unsupervised learning, on the other hand, deals with unlabeled data. The goal is to uncover hidden patterns or structures in the data without explicit feedback. Techniques like clustering and dimensionality reduction fall under this category. Examples include customer segmentation and anomaly detection.

Reinforcement learning is a different paradigm where an agent learns to make decisions by interacting with an environment. It receives rewards or penalties based on its actions and aims to maximize cumulative reward over time. This approach is commonly used in robotics, game playing, and self-driving cars.

Key concepts in machine learning include training and testing data, overfitting, underfitting, model evaluation, and generalization. Feature engineering, data preprocessing, and model selection are critical steps in building effective models.

As machine learning continues to evolve, it is important to consider ethical implications, including data privacy, bias in algorithms, and transparency of decision-making processes. Responsible AI development aims to ensure that these technologies are fair, accountable, and aligned with human values.
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()

tokenizer.fit_on_texts([doc])

len(tokenizer.word_index)

tokenizer.word_index

for sentence in doc.split(
    '\n'
):
  print(tokenizer.texts_to_sequences([sentence])[0])

input_sequences = []

for sentence in doc.split(
    '\n'
):
  tokenized_sen = tokenizer.texts_to_sequences([sentence])[0]

  for i in range(1 , len(tokenized_sen)):
    input_sequences.append(tokenized_sen[:i+1])

input_sequence

max_len = max([len(x) for x in input_sequences])

from tensorflow.keras.preprocessing.sequence import pad_sequences

input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')

input_sequences

x = input_sequences[:,:-1]

y= input_sequences[:,-1]

x.shape

y.shape

from tensorflow.keras.utils import to_categorical



to_categorical(y, num_classes=len(tokenizer.word_index) + 1  )

len(tokenizer.word_index) + 1

from tensorflow.keras.utils import to_categorical
y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)  # Now shape = (259, 173)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM , Dense , Embedding

model = Sequential()

vocab_size = len(tokenizer.word_index) + 1
model.add(Embedding(vocab_size, 100, input_length=max_seq_len - 1))
model.add(LSTM(128))
model.add(Dense(173, activation='softmax'))

model.compile(optimizer='adam' , loss='categorical_crossentropy' , metrics=['accuracy'])

model.summary()

model.fit(x, y, epochs=100, verbose=1)

max_seq_len = max([len(seq) for seq in input_sequences])

def predict_next_word(model , tokenizer , seed_text , max_sequence_len):

  token_list = tokenizer.texts_to_sequences([seed_text])[0]
  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1 ,padding='pre')

  predicted_probs = model.predict(token_list , verbose=3)
  predicted_index = np.argmax(predicted_probs, axis=-1)[0]

  for word , index  in tokenizer.word_index.items():
    if index == predicted_index:
      return word

  return ""

def generate_text(seed_text , next_words, model , tokenizer , max_seq_len):
  for _ in range(next_words):
    next_words = predict_next_word(model, tokenizer , seed_text , max_seq_len)
    seed_text += " " + next_words

  return seed_text

seed = 'machine learning'

generate_text(seed , 10 , model , tokenizer, max_seq_len)

seed = "machine learning"
generated = generate_text(seed, 10, model, tokenizer, max_seq_len)
print(generated)

seed = "unsupervised learning is"
generated = generate_text(seed, 20, model, tokenizer, max_seq_len)
print(generated)
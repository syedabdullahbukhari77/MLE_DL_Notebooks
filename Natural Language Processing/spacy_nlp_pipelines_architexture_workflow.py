# -*- coding: utf-8 -*-
"""NLP _ Pipelines.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iiOThV4DVehHyqT6d17U5xPVA-cyA7oN
"""

import spacy

nlp = spacy.load('en_core_web_sm')

nlp.pipe_names

nlp.pipeline

for name , component in nlp.pipeline: print(f'{name}: {component}')

import spacy
from spacy.language import Language
from spacy.tokens import Doc

nlp = spacy.load('en_core_web_sm')

doc = nlp('Is this a question?')

# Register the custom extension
Doc.set_extension("is_question", default=False)

@Language.component('question_detector')

def question_detector(doc):
  doc.is_question = doc.text.strip().endswith('?')
  return doc

if "question_detector" in nlp.pipe_names:
    nlp.remove_pipe("question_detector")
nlp.add_pipe("question_detector", last=True)


print(doc.is_question)

import spacy
from spacy.language import Language
from spacy.tokens import Doc

# Register a custom extension on the Doc (only once)
if not Doc.has_extension("is_question"):
    Doc.set_extension("is_question", default=False)

# Define the custom component
@Language.component("question_detector")
def question_detector(doc):
    doc._.is_question = doc.text.strip().endswith("?")
    return doc

# Load the model
nlp = spacy.load("en_core_web_sm")

# Add the component only if it's not already in the pipeline
if "question_detector" not in nlp.pipe_names:
    nlp.add_pipe("question_detector", last=True)

# Process the text
doc = nlp("Is this working correctly?")

# Access the custom attribute
print(doc._.is_question)  # âœ… This works!

import spacy
from spacy.language import Language
from spacy.tokens import Doc

Doc.set_extension('is_question', default=False , force=True)

@Language.component('question_detector')

def question_detector(doc):
  doc.is_question = doc.text.strip().endswith('?')
  return doc

nlp = spacy.load('en_core_web_sm')

doc = nlp('Is this working???')

print(f'Is question( True / False ): {doc._.is_question}')

for doc in nlp.pipe(doc , batch_size=2):
  y = [ent.text for ent in doc.ents]
  print (y)

from spacy.language import Language

nlp = Language() #start with a blank Language Class
nlp.add_pipe('sentencizer')

doc = nlp('This is sentence one. This is sentence two.')

for sent in doc.sents:
  print(sent.text)

import spacy
from spacy.language import (Language)
from spacy import (displacy)

nlp = spacy.load('en_core_web_sm')

@Language.component('question_detector')

def question_detector(doc):
  doc._.is_question = doc.text.strip().endswith('?')
  return doc

from spacy.tokens import (Doc)

Doc.set_extension('is_question' , default=False , force=True)

nlp.add_pipe('question_detector' , before='ner')

texts= [
    'Is Google based in US?',
    'Facebook was founded in 2004',
    'How much is Apple worth today?'
]

for doc in nlp.pipe(texts , batch_size=2):
  print(f'Text: {doc.text}')
  print(f'Is Quetsion: {doc._.is_question}')

  for ent in doc.ents:
    print(f' -- {ent.text}({ent.label_})')\

  for token in doc:
    print(f' {token.text:12} POS: {token.pos_:6}')

  displacy.render(doc , style='ent' , jupyter=True)

html = displacy.render(doc , style='ent', jupyter=False)

with open('output.html' , 'w' , encoding='utf-8') as f:
  f.write(html)

nlp = spacy.load('en_core_web_sm')

from fastapi import (FastAPI , Request)
import spacy

nlp = spacy.load('en_core_web_sm')

app = FastAPI()

@app.post('/analyze/')
async def analyze(request: Request):
  data = await request.json()
  text = data.get('text')
  doc = nlp(text)
  return {
      'entities': [(ent.text , ent.label_) for ent in doc.ents]
  }

